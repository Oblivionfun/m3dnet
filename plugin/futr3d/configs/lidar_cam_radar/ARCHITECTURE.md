# FUTR3D ä¸‰ä¼ æ„Ÿå™¨èåˆæ¶æ„è¯¦è§£

## ğŸ—ï¸ æ•´ä½“æ¶æ„æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         è¾“å…¥æ•°æ® (NuScenes Dataset)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“· Camera: 6 views Ã— (3, 1600, 900)                                        â”‚
â”‚  ğŸ¯ LiDAR:  Point Cloud (~30k points Ã— 5 dims) + 9 sweeps                  â”‚
â”‚  ğŸ“¡ Radar:  Sparse Points (~100-1200 Ã— 6 dims) + 4 sweeps                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ç‰¹å¾æå–å™¨ (Parallel Encoding)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ğŸ“· Camera Path     â”‚   ğŸ¯ LiDAR Path      â”‚   ğŸ“¡ Radar Path               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GridMask (Aug)       â”‚ Voxelization         â”‚ Voxelization                  â”‚
â”‚      â†“               â”‚ (0.075, 0.075, 0.2)  â”‚ (0.8, 0.8, 8)                 â”‚
â”‚ VoVNet-99-eSE        â”‚      â†“               â”‚      â†“                        â”‚
â”‚ (ImageNet Pretrain)  â”‚ HardSimpleVFE        â”‚ RadarFeatureNet               â”‚
â”‚      â†“               â”‚ (5D â†’ 128D)          â”‚ (6D â†’ 32D â†’ 64D)              â”‚
â”‚ FPN (4 levels)       â”‚      â†“               â”‚      â†“                        â”‚
â”‚      â†“               â”‚ SparseEncoder        â”‚ PointPillarsScatter           â”‚
â”‚ Output:              â”‚ (3D Sparse Conv)     â”‚      â†“                        â”‚
â”‚ 4 Ã— 256D Ã— 6 views   â”‚      â†“               â”‚ Output:                       â”‚
â”‚                      â”‚ SECOND Backbone      â”‚ 1 Ã— 64D                       â”‚
â”‚                      â”‚ (2D Dense Conv)      â”‚                               â”‚
â”‚                      â”‚      â†“               â”‚                               â”‚
â”‚                      â”‚ FPN (4 levels)       â”‚                               â”‚
â”‚                      â”‚      â†“               â”‚                               â”‚
â”‚                      â”‚ Output:              â”‚                               â”‚
â”‚                      â”‚ 4 Ã— 256D             â”‚                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FUTR3D Transformer Decoder (6 Layers)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: 900 Query Embeddings (256D) + Reference Points (3D)                â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Layer 1-6: Decoder Layer                                          â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  [1] Self-Attention                                                 â”‚   â”‚
â”‚  â”‚      â””â”€> MultiheadAttention (900 queries attend to each other)     â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  [2] Cross-Attention (FUSION POINT) â­                              â”‚   â”‚
â”‚  â”‚      â””â”€> FUTR3DAttention (Multi-Sensor Fusion)                     â”‚   â”‚
â”‚  â”‚          â”œâ”€ ğŸ“· Camera Branch:                                       â”‚   â”‚
â”‚  â”‚          â”‚   - Project 3D queries to 2D image space                â”‚   â”‚
â”‚  â”‚          â”‚   - Grid sample from 6 views Ã— 4 levels                 â”‚   â”‚
â”‚  â”‚          â”‚   - Learned attention weights per camera                â”‚   â”‚
â”‚  â”‚          â”‚   - Output: 900 Ã— 256D                                  â”‚   â”‚
â”‚  â”‚          â”‚                                                          â”‚   â”‚
â”‚  â”‚          â”œâ”€ ğŸ¯ LiDAR Branch:                                        â”‚   â”‚
â”‚  â”‚          â”‚   - Multi-scale deformable attention                    â”‚   â”‚
â”‚  â”‚          â”‚   - Adaptive sampling from 4 FPN levels                 â”‚   â”‚
â”‚  â”‚          â”‚   - Learned offsets + attention weights                 â”‚   â”‚
â”‚  â”‚          â”‚   - Output: 900 Ã— 256D                                  â”‚   â”‚
â”‚  â”‚          â”‚                                                          â”‚   â”‚
â”‚  â”‚          â”œâ”€ ğŸ“¡ Radar Branch:                                        â”‚   â”‚
â”‚  â”‚          â”‚   - Multi-scale deformable attention                    â”‚   â”‚
â”‚  â”‚          â”‚   - Sampling from radar feature map                     â”‚   â”‚
â”‚  â”‚          â”‚   - Velocity-aware feature encoding                     â”‚   â”‚
â”‚  â”‚          â”‚   - Output: 900 Ã— 64D                                   â”‚   â”‚
â”‚  â”‚          â”‚                                                          â”‚   â”‚
â”‚  â”‚          â””â”€ Fusion Layer:                                          â”‚   â”‚
â”‚  â”‚              Concat(ğŸ“·256D + ğŸ¯256D + ğŸ“¡64D) = 576D                â”‚   â”‚
â”‚  â”‚                        â†“                                           â”‚   â”‚
â”‚  â”‚              MLP Projection: 576D â†’ 256D                           â”‚   â”‚
â”‚  â”‚              LayerNorm â†’ ReLU â†’ Linear â†’ LayerNorm                 â”‚   â”‚
â”‚  â”‚                        â†“                                           â”‚   â”‚
â”‚  â”‚              Output: 900 Ã— 256D (fused features)                   â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  [3] Feed Forward Network (FFN)                                    â”‚   â”‚
â”‚  â”‚      â””â”€> MLP: 256D â†’ 1024D â†’ 256D                                  â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  [4] Box Refinement (DAB - Dynamic Anchor Boxes)                   â”‚   â”‚
â”‚  â”‚      â””â”€> Update reference points based on predictions              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Intermediate Outputs: 6 layers Ã— (900 Ã— 256D)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Detection Heads (per layer)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Classification Head:                                                       â”‚
â”‚  900 Ã— 256D â†’ Linear â†’ 900 Ã— 10 (class logits)                             â”‚
â”‚                                                                             â”‚
â”‚  Regression Head:                                                           â”‚
â”‚  900 Ã— 256D â†’ Linear â†’ 900 Ã— 9                                             â”‚
â”‚  (x, y, z, w, h, l, sin(Î¸), cos(Î¸), vz)                                    â”‚
â”‚                                                                             â”‚
â”‚  â”œâ”€ Loss Functions:                                                         â”‚
â”‚  â”‚  - Focal Loss (classification)                                          â”‚
â”‚  â”‚  - L1 Loss (bbox regression)                                            â”‚
â”‚  â”‚  - GIoU Loss (bbox overlap)                                             â”‚
â”‚  â””â”€ Hungarian Matching (bipartite matching)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Final Output (Post-Processing)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Top-300 Detections:                                                        â”‚
â”‚  - 3D Bounding Boxes: (x, y, z, w, h, l, Î¸, vz)                            â”‚
â”‚  - Class Probabilities: 10 classes                                          â”‚
â”‚  - Confidence Scores: [0, 1]                                                â”‚
â”‚                                                                             â”‚
â”‚  NMS (Non-Maximum Suppression):                                             â”‚
â”‚  - Circle NMS (Bird's Eye View)                                             â”‚
â”‚  - IoU threshold: 0.2                                                       â”‚
â”‚  - Final outputs: ~83 objects per scene                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” å…³é”®ç»„ä»¶è¯¦è§£

### 1. Cameraç‰¹å¾æå–

```python
Input: (B, 6, 3, 1600, 900)  # 6 camera views
    â†“ GridMask augmentation
    â†“ VoVNet-99-eSE backbone
    â”œâ”€ stage2: 256 channels
    â”œâ”€ stage3: 512 channels
    â”œâ”€ stage4: 768 channels
    â””â”€ stage5: 1024 channels
    â†“ FPN neck
Output: 4 levels Ã— (B, 6, 256, H/8, W/8) to (B, 6, 256, H/64, W/64)
```

**ç‰¹ç‚¹:**
- **VoVNet:** One-Shot Aggregation architectureï¼Œè®¡ç®—é«˜æ•ˆ
- **FPN:** å¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”ï¼Œæ•è·ä¸åŒå¤§å°çš„ç‰©ä½“
- **6è§†è§’èåˆ:** Front, Front-Left, Front-Right, Back, Back-Left, Back-Right

### 2. LiDARç‰¹å¾æå–

```python
Input: (B, ~30k, 5)  # x, y, z, intensity, timestamp
    â†“ Voxelization: (0.075m, 0.075m, 0.2m)
    â†’ Sparse voxels: (B, ~120k, 128D)
    â†“ SparseEncoder (3D Sparse Convolution)
    â†’ Feature volume: (B, 128, 1440, 1440)
    â†“ SECOND backbone (2D Dense Convolution)
    â”œâ”€ layer1: 128 channels
    â””â”€ layer2: 256 channels
    â†“ FPN neck
Output: 4 levels Ã— (B, 256, H, W)
```

**ç‰¹ç‚¹:**
- **é«˜ç²¾åº¦ä½“ç´ åŒ–:** 0.075m åˆ†è¾¨ç‡ï¼Œæ•è·ç²¾ç»†å‡ ä½•ç»“æ„
- **ç¨€ç–å·ç§¯:** å¤„ç†ç¨€ç–3Dæ•°æ®ï¼Œè®¡ç®—é«˜æ•ˆ
- **æ—¶åºèšåˆ:** 9å¸§ç‚¹äº‘èåˆï¼Œå¢åŠ ç‚¹äº‘å¯†åº¦

### 3. Radarç‰¹å¾æå–

```python
Input: (B, ~1200, 6)  # x, y, z, rcs, vx, vy
    â†“ Voxelization: (0.8m, 0.8m, 8m)
    â†’ Sparse voxels: (B, ~90k, 64D)
    â†“ RadarFeatureNet
    â”œâ”€ Linear: 6D â†’ 32D
    â””â”€ Linear: 32D â†’ 64D
    â†“ PointPillarsScatter
Output: 1 level Ã— (B, 64, 128, 128)
```

**ç‰¹ç‚¹:**
- **é€Ÿåº¦ä¿¡æ¯:** ç›´æ¥æµ‹é‡ç‰©ä½“å¾„å‘é€Ÿåº¦ (vx, vy)
- **RCS (Radar Cross Section):** ç‰©ä½“åå°„å¼ºåº¦
- **ç²—ç²’åº¦ä½“ç´ åŒ–:** é€‚åº”é›·è¾¾ç¨€ç–ç‰¹æ€§

---

## âš™ï¸ FUTR3DAttention èåˆæœºåˆ¶

### æ³¨æ„åŠ›å…¬å¼

#### Cameraåˆ†æ”¯ (Grid Sampling)

```
1. 3D â†’ 2D Projection:
   P_cam = K Ã— [R|t] Ã— P_3d
   where P_3d: (900, 3) query positions
         P_cam: (900, 6, 2) image coordinates

2. Feature Sampling:
   F_cam = GridSample(Image_Features, P_cam)
   F_cam: (900, 6, 4_levels, 256)

3. Attention Weighting:
   W_cam = Sigmoid(Linear(Query))  # (900, 6Ã—4)
   Output_cam = Sum(F_cam Ã— W_cam)  # (900, 256)
```

#### LiDARåˆ†æ”¯ (Deformable Attention)

```
1. Multi-Scale Sampling:
   Î”p_i = Linear(Query)  # (900, 4_levels Ã— 4_points, 2)
   p_i = p_ref + Î”p_i    # Adaptive offset

2. Feature Sampling:
   F_lidar = Î£_m Î£_k W_mk Ã— Sample(Feature_m, p_mk)
   where m: FPN level, k: sampling point

3. Output Projection:
   Output_lidar = Linear(F_lidar)  # (900, 256)
```

#### Radaråˆ†æ”¯ (ç±»ä¼¼LiDAR)

```
Output_radar = DeformableAttention(Radar_Features)  # (900, 64)
```

#### èåˆå±‚

```
1. Concatenation:
   F_concat = Concat(Output_cam, Output_lidar, Output_radar)
   F_concat: (900, 256+256+64) = (900, 576)

2. MLP Projection:
   F_fused = MLP(F_concat)
   F_fused = LayerNorm(Linear(F_concat))  # 576 â†’ 256
   F_fused = ReLU(F_fused)
   F_fused = LayerNorm(Linear(F_fused))   # 256 â†’ 256

   Output: (900, 256)
```

---

## ğŸ“Š å‚æ•°ç»Ÿè®¡

### æ¨¡å‹å¤§å°

| ç»„ä»¶ | å‚æ•°é‡ | å¤‡æ³¨ |
|------|--------|------|
| Camera Backbone (VoVNet) | ~99M | ImageNeté¢„è®­ç»ƒ |
| Camera Neck (FPN) | ~2M | 4å±‚FPN |
| LiDAR Encoder | ~15M | Sparse + SECOND |
| LiDAR Neck (FPN) | ~1M | 4å±‚FPN |
| Radar Encoder | ~0.5M | è½»é‡çº§ç½‘ç»œ |
| FUTR3D Head | ~10M | Transformer + Heads |
| **Total** | **~127M** | æ¯”åŒä¼ æ„Ÿå™¨å¤š 0.5M |

### è®¡ç®—å¤æ‚åº¦ (FLOPs)

| ç»„ä»¶ | FLOPs | å æ¯” |
|------|-------|------|
| Camera Path | ~350 GFLOPs | 45% |
| LiDAR Path | ~300 GFLOPs | 38% |
| Radar Path | ~15 GFLOPs | 2% |
| Transformer | ~120 GFLOPs | 15% |
| **Total** | **~785 GFLOPs** | - |

### å†…å­˜å ç”¨ (å•GPU, batch_size=1)

| ç»„ä»¶ | æ˜¾å­˜ (GB) |
|------|-----------|
| Camera Features | ~4.5 |
| LiDAR Features | ~3.8 |
| Radar Features | ~0.2 |
| Transformer States | ~2.5 |
| Gradients + Optimizer | ~11.0 |
| **Total** | **~22 GB** |

---

## ğŸ¯ èåˆç­–ç•¥å¯¹æ¯”

### æ—©æœŸèåˆ (Early Fusion)
```
Camera â†’
LiDAR  â†’ Concat â†’ Encoder â†’ Decoder â†’ Detection
Radar  â†’
```
**ç¼ºç‚¹:** ä¼ æ„Ÿå™¨ç‰¹æ€§éš¾ä»¥å……åˆ†åˆ©ç”¨

### æ™šæœŸèåˆ (Late Fusion)
```
Camera â†’ Encoder â†’ Detection â†˜
LiDAR  â†’ Encoder â†’ Detection â†’ Ensemble â†’ Final
Radar  â†’ Encoder â†’ Detection â†—
```
**ç¼ºç‚¹:** ç¼ºä¹è·¨æ¨¡æ€äº¤äº’

### FUTR3Dèåˆ (Attention-based Fusion) â­
```
Camera â†’ Encoder â†˜
LiDAR  â†’ Encoder â†’ Cross-Attention (Fusion) â†’ Detection
Radar  â†’ Encoder â†—
```
**ä¼˜ç‚¹:**
- âœ… ä¿ç•™æ¨¡æ€ç‰¹æ€§
- âœ… è‡ªé€‚åº”èåˆæƒé‡
- âœ… è·¨æ¨¡æ€äº¤äº’
- âœ… ç«¯åˆ°ç«¯è®­ç»ƒ

---

## ğŸ”¬ è®¾è®¡å†³ç­–

### ä¸ºä»€ä¹ˆ Radar ç»´åº¦æ˜¯ 64Dï¼Ÿ

**åŸå› :**
1. Radaræ•°æ®ç¨€ç– (~100-1200ç‚¹ vs LiDAR ~30kç‚¹)
2. é™ä½è®¡ç®—å¤æ‚åº¦
3. ä¸»è¦æä¾›é€Ÿåº¦ä¿¡æ¯ï¼Œä¸éœ€è¦é«˜ç»´ç‰¹å¾

### ä¸ºä»€ä¹ˆ LiDAR ä½“ç´ æ›´å°ï¼Ÿ

**åŸå› :**
1. LiDARç‚¹äº‘å¯†é›†ï¼Œéœ€è¦ç²¾ç»†å‡ ä½•
2. 0.075m èƒ½æ•è·è½¦è¾†ã€è¡Œäººç­‰å°ç›®æ ‡
3. Radarç¨€ç–ï¼Œ0.8m ä½“ç´ å·²è¶³å¤Ÿ

### ä¸ºä»€ä¹ˆä½¿ç”¨ 900 ä¸ª Queryï¼Ÿ

**åŸå› :**
1. è¦†ç›– 108mÃ—108m æ£€æµ‹èŒƒå›´
2. æ¯ä¸ª query çº¦è´Ÿè´£ 13mÂ² åŒºåŸŸ
3. å¹³è¡¡æ£€æµ‹èƒ½åŠ›å’Œè®¡ç®—æˆæœ¬

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ··åˆç²¾åº¦è®­ç»ƒ

```python
fp16 = dict(loss_scale=512.)
```
**æ”¶ç›Š:** å‡å°‘ 30-40% æ˜¾å­˜ï¼ŒåŠ é€Ÿ 20-30%

### 2. æ¢¯åº¦ç´¯ç§¯

```python
optimizer_config = dict(
    grad_clip=dict(max_norm=35, norm_type=2),
    cumulative_iters=2)  # æœ‰æ•ˆ batch size Ã— 2
```
**é€‚ç”¨:** æ˜¾å­˜ä¸è¶³æ—¶

### 3. æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)

```python
# åœ¨ backbone ä¸­å¯ç”¨
pts_backbone=dict(
    type='SECOND',
    with_cp=True)  # å¯ç”¨æ£€æŸ¥ç‚¹
```
**æ”¶ç›Š:** å‡å°‘ 40% æ˜¾å­˜ï¼Œè®­ç»ƒé€Ÿåº¦ -15%

---

## ğŸ“ˆ å¯æ‰©å±•æ€§

### æ·»åŠ ç¬¬å››ç§ä¼ æ„Ÿå™¨ (å¦‚æ¯«ç±³æ³¢é›·è¾¾)

åªéœ€ï¼š
1. æ·»åŠ ç‰¹å¾æå–å™¨é…ç½®
2. åœ¨ `FUTR3DAttention` ä¸­æ·»åŠ æ–°åˆ†æ”¯
3. æ›´æ–° `Collect3D` keys

### æ”¯æŒå…¶ä»–æ•°æ®é›†

ä¿®æ”¹ï¼š
1. `point_cloud_range`
2. `voxel_size`
3. `class_names`
4. æ•°æ®åŠ è½½ pipeline

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

- **FUTR3D:** [arXiv:2203.10642](https://arxiv.org/abs/2203.10642)
- **VoVNet:** "An Energy and GPU-Computation Efficient Backbone Network"
- **Deformable DETR:** "Deformable Transformers for End-to-End Object Detection"
- **SECOND:** "Sparsely Embedded Convolutional Detection"

